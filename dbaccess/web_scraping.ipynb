{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e11237f-cdeb-4555-a541-55bc9545cd57",
   "metadata": {},
   "source": [
    "# Biomedical Data Bases, 2021-2022\n",
    "### Web Scraping\n",
    "These are notes by prof. Davide Salomoni (d.salomoni@unibo.it) for the Biomedical Data Base course at the University of Bologna, academic year 2021-2022.\n",
    "\n",
    "We will here demonstrate with a simple example how to fetch and process some data, related in particular to posting of bioinformatics jobs. Study the slides for more details and remember **not to overuse** web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0baaa29-2011-4863-aa8d-24b3cf641f79",
   "metadata": {},
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d014b-4231-4f09-9850-4b4abc46f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d37d9-a19d-417d-a3f6-a1398bf533ea",
   "metadata": {},
   "source": [
    "## Example 1: scraping bioinformatics jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ae472-853a-4076-805a-024e5996451d",
   "metadata": {},
   "source": [
    "### Get the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582592fa-0f72-4d84-a1ed-464624d9781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 10 # number of jobs to retrieve\n",
    "JOB_URL = 'https://www.bioinformatics.org/jobs/?group_id=101&summaries=1&length=%s' % (LENGTH-1)\n",
    "\n",
    "# get the HTML of the page with requests.get()\n",
    "r = requests.get(JOB_URL)\n",
    "\n",
    "# now pass the page content to bs4\n",
    "soup = BeautifulSoup(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaaaf20-b7b6-47e7-93f5-18e69053ba9f",
   "metadata": {},
   "source": [
    "### Get the elements that interest us\n",
    "\n",
    "Looking at the HTML of the web page, it turns out that the job opportunities are in some _tables_. But the web page has many tables: how to find the ones that are interesting for us? We notice that the job opportunities are in tables where the text \"Opportunity\" is part of the content.\n",
    "\n",
    "So, we get first all the tables, and then we retain only those with \"Opportunity\" in the content, storing them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada67c1f-664d-4a80-adcd-e9aec95e129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the tables\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# store the interesting tables in a list\n",
    "entries = list()\n",
    "for table in tables:\n",
    "    if 'Opportunity' in table.text:\n",
    "        entries.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f8498-d3f4-451b-a389-895a0569f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we look at the list 'entries', we realize that the first two tables are not interesting (check for yourself the content of entries[0] and entries[1])\n",
    "# therefore, we remove the first two items of the list and retain the remaining items\n",
    "entries = entries[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1dfef0-a3db-4dfb-b5e8-5f76011c4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what an entry looks like, using entries[0] as an example\n",
    "entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54164e7-7bd3-4ce0-8387-715d27924831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each entry is a BeatifulSoup structure\n",
    "type(entries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf3d51-a630-4119-98f9-dfc5e601de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entries[0] looks OK. We can apply several bs4 methods to it. For instance, let's see the text\n",
    "entries[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4222d6b-0be8-43d5-b05b-383b6d822e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, there is some parsing to do on the text to remove unwanted elements, such as those \\n characters. We will do it later.\n",
    "\n",
    "# Let's now use the bs4 find_all method to find the 'a' HTML tags (identifying links)\n",
    "links = entries[0].find_all('a', href=True)\n",
    "\n",
    "# check what we got\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ddcf9-35c5-4dfe-93bc-cb9f475a3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got a list, of which we want to retain only the first element (the second refers to the person who posted the job opportunity)\n",
    "\n",
    "# let's print out the \"href\", i.e. the actual link of the HTML tag\n",
    "for link in links:\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee87fa4-485d-43d5-b42f-fce701f83608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, if we want only the first element:\n",
    "print(links[0]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91212f1d-b170-4339-83cf-8df6b6812663",
   "metadata": {},
   "source": [
    "### Parse the text and store it in a list\n",
    "\n",
    "Now that we have understood how to process the various elements of the HTML page, let's put everything together.\n",
    "\n",
    "We will parse the text of each item of the 'entries' list using some Python constructs, as well as a _regular expression_. Let's look at the text for an item (see above); it could something like this:\n",
    "\n",
    "```\n",
    "'\\n\\n\\n\\nOpportunity: Bioinformatics Data Analyst @ Bowie State University -- Bowie, MD (US)\\nSubmitted by Konda Reddy Karnati; posted on Friday,\\xa0January\\xa028,\\xa02022 \\n\\n\\n\\n'\n",
    "```\n",
    "\n",
    "- we use strip() to remove the \\n characters at the beginning and at the end of each line\n",
    "- the text contained between ':' and '@' is the job title\n",
    "- the text contained between '@' and '\\n' is the job location\n",
    "- the text after 'posted on ' is the publication data. Here we also need to replace \\xa0 with a regular space\n",
    "\n",
    "Also, a link will be something like this (see above):\n",
    "\n",
    "```\n",
    "https://www.bioinformatics.org/forums/forum.php?forum_id=14619\n",
    "```\n",
    "\n",
    "- the text after 'forum_id=' seems to be the job number, so we will extract that as well\n",
    "\n",
    "We will put all the extracted elements in the list called `my_jobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857dac10-efc5-4169-a9e5-17fe06def29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_jobs = list()\n",
    "for entry in entries:\n",
    "    link = entry.find_all('a', href=True)[0]['href'] # we keep only the first link (see above)\n",
    "    text = entry.text.strip()\n",
    "    \n",
    "    # parse the text, grouping the interesting parts as explained above\n",
    "    # look up 'regular expressions in Python' if you are not familiar with the re.search syntax \n",
    "    m = re.search('Opportunity: (.+?) @ (.+?)\\n.+?; posted on (.+?)$', text)\n",
    "    \n",
    "    # extract the various groups of the regular expression\n",
    "    title = m.group(1)\n",
    "    location = m.group(2)\n",
    "    date = m.group(3).replace('\\xa0', ' ') # replace also \\xa0 with a space\n",
    "    \n",
    "    # extract the job id from the link\n",
    "    m = re.search('.*forum_id=(.+)$', link)\n",
    "    job_id = m.group(1)\n",
    "    \n",
    "    # finally, append all the extracted elements to a list\n",
    "    my_jobs.append([job_id, title, location, link, date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4193a-05c6-4f5e-b80b-7f5fa7059b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what we got in the end\n",
    "my_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7709e1-fe6a-4b89-b65f-f992e911742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print it with some formatting\n",
    "for job in my_jobs:\n",
    "    print(\"Job number: %s\" % job[0])\n",
    "    print(\"\\tTitle: %s\" % job[1])\n",
    "    print(\"\\tLocation: %s\" % job[2])\n",
    "    print(\"\\tURL: %s\" % job[3])\n",
    "    print(\"\\tPublished on: %s\\n\" % job[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfba4b-ca37-41a2-922e-9ce64a4d0c68",
   "metadata": {},
   "source": [
    "### Get information from a secondary page\n",
    "\n",
    "You can apply web scraping also to the pages detailing each job. For example, let's go through the list of jobs we have retrieved so far, visit the respective URLs, and fetch the \"DEADLINE\". We will store this deadline into a python dictionary called `my_deadlines`; in this dictionary, we will define the key to be the job ID (which looks like a unique key for each job), and as value the deadline.\n",
    "\n",
    "Looking the HTML for one of the URLs, we notice that the deadline is contained in a class called `sf-news`, and that the text of that class is \"DEADLINE\". There might be more than one instance of the `sf-news` class, so we loop through all of them, and stop when we find the one with the DEADLINE text. The actual deadline is container in the next element, which can be found in the `next_siblings` attribute of the class.\n",
    "\n",
    "The deadline string will have some \\r or \\n characters before and after it, so we will remove them with `strip()`.\n",
    "\n",
    "__However__, note that the \"DEADLINE\" field is _not_ mandatory, so it might not be present on a page. We should therefore consider also this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428ceca-be2e-4de4-9521-55acfc682bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify how a job looks like, printing for example my_jobs[0]\n",
    "my_jobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e39f8-b100-4681-a5dc-7cf3def7fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deadlines = dict()\n",
    "for job in my_jobs:\n",
    "    job_id = job[0]\n",
    "    job_link = job[3]\n",
    "    \n",
    "    # get the page referenced by the current job\n",
    "    r = requests.get(job_link)\n",
    "    \n",
    "    # pass the page content to bs4\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    \n",
    "    # find all the \"sf-news\" classes and get the one with text \"DEADLINE\"\n",
    "    for c in soup.find_all(class_ = 'sf-news'):\n",
    "        if c.text == \"DEADLINE\":\n",
    "            # get the deadline, create a dictionary item and then exit from this loop\n",
    "            my_deadlines[job_id] = c.next_sibling.strip()\n",
    "            break\n",
    "    else:\n",
    "        # this part gets executed if no \"break\" was encountered.\n",
    "        my_deadlines[job_id] = 'No deadline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8511ff-544b-4383-8e2f-81e0c9291362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that my_deadlines contains what we want\n",
    "my_deadlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19484403-5c1d-48cd-a2d6-8b2f0cd93c91",
   "metadata": {},
   "source": [
    "### Combine everything into a single block of code and store the info in a database\n",
    "\n",
    "In the cells above, we first created the list `my_jobs` and then the dictionary `my_deadlines` to try out the code. Now that we have it working, we can combine everything into a single block of code, and for example print out all the information we have gathered.\n",
    "\n",
    "We could then store everything in a permanent place, for example in a database. __Try this out for yourself__, for instance using a Redis database, configured with persistency. Then see if you can retrieve the info from the DB using various types of searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2aff4-17da-4417-bd4b-5a0e061b7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would be the final code, from start to end\n",
    "\n",
    "LENGTH = 10 # number of jobs to retrieve\n",
    "JOB_URL = 'https://www.bioinformatics.org/jobs/?group_id=101&summaries=1&length=%s' % (LENGTH-1)\n",
    "\n",
    "# get the HTML of the page with requests.get()\n",
    "r = requests.get(JOB_URL)\n",
    "\n",
    "# now pass the page content to bs4\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# get all the tables\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# store the interesting tables in a list\n",
    "entries = list()\n",
    "for table in tables:\n",
    "    if 'Opportunity' in table.text:\n",
    "        entries.append(table)\n",
    "\n",
    "# remove the first two entries\n",
    "entries = entries[2:]\n",
    "\n",
    "for entry in entries:\n",
    "    link = entry.find_all('a', href=True)[0]['href'] # we keep only the first link (see above)\n",
    "    text = entry.text.strip()\n",
    "    \n",
    "    # parse the text, grouping the interesting parts as explained above\n",
    "    # look up 'regular expressions in Python' if you are not familiar with the re.search syntax \n",
    "    m = re.search('Opportunity: (.+?) @ (.+?)\\n.+?; posted on (.+?)$', text)\n",
    "    \n",
    "    # extract the various groups of the regular expression\n",
    "    title = m.group(1)\n",
    "    location = m.group(2)\n",
    "    date = m.group(3).replace('\\xa0', ' ') # replace also \\xa0 with a space\n",
    "    \n",
    "    # extract the job id from the link\n",
    "    m = re.search('.*forum_id=(.+)$', link)\n",
    "    job_id = m.group(1)\n",
    "    \n",
    "    # get the deadline by scraping the link\n",
    "    link_page = requests.get(link)\n",
    "    # pass the page content to bs4\n",
    "    link_soup = BeautifulSoup(link_page.content)\n",
    "    \n",
    "    # find all the \"sf-news\" classes and get the one with text \"DEADLINE\"\n",
    "    for c in link_soup.find_all(class_ = 'sf-news'):\n",
    "        if c.text == \"DEADLINE\":\n",
    "            # get the deadline text. remove spurious characters and exit from this loop\n",
    "            deadline = c.next_sibling.strip()\n",
    "            break\n",
    "    else:\n",
    "        # # this part gets executed if no \"break\" was encountered.\n",
    "        deadline = 'No deadline'\n",
    "   \n",
    "    # print out all that we have gathered, with some formatting\n",
    "    print(\"Job number: %s\" % job_id)\n",
    "    print(\"\\tTitle: %s\" % title)\n",
    "    print(\"\\tLocation: %s\" % location)\n",
    "    print(\"\\tURL: %s\" % link)\n",
    "    print(\"\\tPublished on: %s\" % date)\n",
    "    print(\"\\tDeadline: %s\\n\" % deadline)\n",
    "    \n",
    "    # here we could insert all the info into a list as we did earlier\n",
    "    # or, instead of creating the list, we could write the info to a DB\n",
    "    # <do something with the info>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecb187-03bf-40e1-be92-0c9bb24a64e5",
   "metadata": {},
   "source": [
    "## Example 2: getting hourly weather forecasts\n",
    "\n",
    "This and the following example are provided without many explanations. You should analyze the code, break it down into multiple cells and verify what each operations does. There are also other ways to extract the same info: you are encouraged to find and test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a4a6a-42c0-4ebe-a8f1-0bd0c29bfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the datetime module to get the current date\n",
    "from datetime import date\n",
    "\n",
    "WEATHER_URL = 'https://www.ilmeteo.it/meteo/Bologna'\n",
    "r = requests.get(WEATHER_URL)\n",
    "soup = BeautifulSoup(r.content) \n",
    "\n",
    "# extract the main table, identified in the HTML code by a class called 'datatable'\n",
    "infos = soup.find_all(class_ = 'datatable')\n",
    "\n",
    "# extract rows and columns of that table, looking at the tags <tr> and <td>\n",
    "data = list()\n",
    "rows = infos[0].find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols):\n",
    "        # just keep the text of each cell\n",
    "        cols = [i.text.strip() for i in cols]\n",
    "        # put everything in data, removing empty entries\n",
    "        data.append([i for i in cols if i])\n",
    "\n",
    "# note that at the beginning of the data list there are several uniteresting elements\n",
    "# let's keep only the list elements where the first element is a number\n",
    "data = [i for i in data if i[0].isdigit()]\n",
    "\n",
    "# create a Pandas data frame from the data list. Column names will be by default 0, 1, 2, ...\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# keep only the column I am interesting in (0, 1, 2) and convert column 2 into float\n",
    "weather = df.iloc[:, [0,1,2]].copy()\n",
    "weather.iloc[:,2] = weather.iloc[:,2].map(lambda x: float(x[:-1]))\n",
    "\n",
    "# see what it looks like\n",
    "print(weather)\n",
    "\n",
    "# plot it \n",
    "my_date = date.today().strftime('%d/%m/%Y') # get today's date in the d/m/Y format\n",
    "weather.plot(x=0,\n",
    "             y=2,\n",
    "             xlabel='Hour',\n",
    "             ylabel='T (°C)',\n",
    "             legend=False,\n",
    "             title='Hourly weather forecast for Bologna on %s' % my_date\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280cb7a-e455-42e8-9c38-9883b8004e1d",
   "metadata": {},
   "source": [
    "## Example 3: getting weekly weather forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0be3f-9f95-4d3d-853e-80616e4159b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_URL = 'https://www.ilmeteo.it/meteo/Bologna'\n",
    "r = requests.get(WEATHER_URL)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# find all the tags like <ul id='daytabs' ...>\n",
    "infos = soup.find_all('ul', id='daytabs')\n",
    "\n",
    "# find all the HTML lists (tag 'li')\n",
    "entries = infos[0].find_all('li')\n",
    "\n",
    "# looking at the entries, we notice that we must remove those that do not start with a weekday\n",
    "days = ['Lun', 'Mar', 'Mer', 'Gio', 'Ven', 'Sab', 'Dom']\n",
    "data = list()\n",
    "for entry in entries:\n",
    "    text = list(entry.text.split())\n",
    "    if text[0] in days:\n",
    "        data.append(text)\n",
    "\n",
    "# the list now looks almost ok, but in order to plot it we must:\n",
    "# 1) join the first two fields: 'Sab' and '7' should become 'Sab 7' \n",
    "# 2) separate the third field: 19°35° should become '19' and '35'\n",
    "for row in data:\n",
    "    one, two, three = row\n",
    "    tmin, tmax, _ = three.split('°')\n",
    "    row[0] = one + ' ' + two\n",
    "    row[1] = int(tmin)\n",
    "    row[2] = int(tmax)\n",
    "\n",
    "# good, we can now create a Pandas data frame from that list\n",
    "df = pd.DataFrame(data, columns = ['Date', 'T(min)', 'T(max)'])\n",
    "print(df)\n",
    "\n",
    "# and finally plot it\n",
    "df.plot(x='Date', \n",
    "        y=['T(min)','T(max)'], \n",
    "        rot=45, \n",
    "        ylabel='T (°C)', \n",
    "        title='Weekly weather forecast for Bologna on %s' % my_date\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a2d13-264c-4d94-9d1f-f66021678cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
